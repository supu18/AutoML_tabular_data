{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ALL TOOLS ON DECK**\n",
    "\n",
    "---\n",
    "AutoGluon is a cutting-edge tool for automating machine learning (AutoML) processes on tabular datasets. The objective is to investigate the performance of several AutoML tools, including Auto-sklearn, FLAML,TPOT and MLJAR in comparison to AutoGluon's predictive capabilities.\n",
    "\n",
    "While AutoGluon typically excels in longer training durations, our focus lies in examining its performance within shorter training times, specifically less than a minute. We aim to contrast this with an AutoGluon predictor trained for an equivalent total duration.\n",
    "\n",
    "# Overview of AutoML Tools\n",
    "## 1. AutoGluon\n",
    "Description: AutoGluon is an open-source AutoML framework from Amazon that focuses on ease of use and high performance. It automates machine learning workflows, including feature engineering, model selection, and hyperparameter tuning.\n",
    "### Strengths:\n",
    "*   Versatility: Supports multiple data types and tasks (e.g., regression, classification).\n",
    "*   Ensemble Learning: Automatically builds ensembles of models.\n",
    "*   Efficiency: Optimized for performance with multi-threading and GPU support.\n",
    "\n",
    "## 2. MLJAR\n",
    "Description: MLJAR is a Python library that automates the machine learning pipeline with a focus on simplicity and interpretability. It also supports multiple types of data and tasks.\n",
    "### Strengths:\n",
    "*   Easy-to-Use Interface: Simplified API for quick model training.\n",
    "*   Ensemble Learning: Combines multiple models to improve performance.\n",
    "*   Feature Importance: Provides insights into feature importance.\n",
    "\n",
    "## 3. TPOT\n",
    "Description: TPOT (Tree-based Pipeline Optimization Tool) is an AutoML tool that uses genetic algorithms to optimize machine learning pipelines. It's part of the scikit-learn ecosystem.\n",
    "### Strengths:\n",
    "\n",
    "\n",
    "*   Pipeline Optimization: Automatically designs and optimizes machine learning pipelines.\n",
    "*   Genetic Algorithms: Uses evolutionary algorithms to find the best models.\n",
    "* Customization: Allows for detailed control over the optimization process.\n",
    "\n",
    "Sure! Here is the information for Auto-sklearn and FLAML in the same format:\n",
    "\n",
    "## 4. Auto-sklearn\n",
    "Description: Auto-sklearn is an open-source AutoML tool built on top of the scikit-learn library. It uses Bayesian optimization to automate the process of model selection and hyperparameter tuning.\n",
    "### Strengths:\n",
    "*   Bayesian Optimization: Efficiently searches the hyperparameter space.\n",
    "*   Meta-Learning: Leverages prior knowledge to warm-start the optimization.\n",
    "*   Ensemble Learning: Automatically constructs ensembles of models to improve performance.\n",
    "*   Extensible: Integrates well with the scikit-learn ecosystem, allowing for custom pipelines and preprocessing.\n",
    "\n",
    "## 5. FLAML\n",
    "Description: FLAML (Fast and Lightweight AutoML) is a lightweight open-source library developed by Microsoft Research. It focuses on efficient and fast AutoML for both classification and regression tasks.\n",
    "### Strengths:\n",
    "*   Efficiency: Optimized for fast performance and low computational cost.\n",
    "*   Simplicity: Easy-to-use interface with minimal setup.\n",
    "*   Customization: Allows users to specify constraints and customize the search space.\n",
    "*   Versatility: Supports a range of machine learning tasks including time series forecasting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install all required packages and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Settings\n",
    "\n",
    "Change random_seed to draw different samples from the dataset. The default value is 42. Change the number of samples loaded for the practice sets by changing it in the loading cell three cells below. The value set now is 20 000 to mimick the exam-set size.\n",
    "\n",
    "Change the time each model is allowed to train by changing the time parameter in the cell below. The default value is 45 seconds as it has shown to be a competetive for time vs performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "time = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load different datsets with respect to the dataset of the experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset\n",
    "\n",
    "base_path = '../../data/exam_dataset/1'\n",
    "\n",
    "X_train = pd.read_parquet(f'{base_path}/X_train.parquet')\n",
    "y_train = pd.read_parquet(f'{base_path}/y_train.parquet')\n",
    "train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "#test = train_dataset.sample(frac=0.2, replace=False, random_state=random_seed)\n",
    "exam_X_values = pd.read_parquet(f'{base_path}/X_test.parquet')\n",
    "\n",
    "# Also instantiate the target column\n",
    "label = 'price'\n",
    "\n",
    "\n",
    "\n",
    "print(train_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path respective to the dataset you are testing:\n",
    "#  * Bike_Sharing_Demand (361099) - label: 'count'\n",
    "#  * Brazilian Houses (361098) - label: 'total_(BRL)'\n",
    "#  * y_prop_4_1 (361092) - label: 'oz252'\n",
    "# Because the exam dataset is ~20,000 entries, we should sample around this as well maybe?\n",
    "\n",
    "# Set the base path for the dataset\n",
    "base_path = '../../data/361099'\n",
    "\n",
    "# Initialize variables for training data\n",
    "X_train = None\n",
    "y_train = None\n",
    "\n",
    "# Loop through each fold of the dataset\n",
    "for fold_number in range(1, 11):\n",
    "    # Read the X_train and y_train data for the current fold\n",
    "    x_fold = pd.read_parquet(f'{base_path}/{fold_number}/X_train.parquet')\n",
    "    y_fold = pd.read_parquet(f'{base_path}/{fold_number}/y_train.parquet')\n",
    "\n",
    "    # Concatenate the data to the existing training data\n",
    "    if X_train is None:\n",
    "        X_train = x_fold\n",
    "        y_train = y_fold\n",
    "    else:\n",
    "        X_train = pd.concat([X_train, x_fold])\n",
    "        y_train = pd.concat([y_train, y_fold])\n",
    "\n",
    "# Sample the training data to around 20,000 entries\n",
    "X_train = X_train.sample(n=20000, random_state=random_seed)\n",
    "y_train = y_train.sample(n=20000, random_state=random_seed)\n",
    "\n",
    "# Create a concatenated dataset for gluon\n",
    "train_dataset = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Initialize variable for test data\n",
    "test = None\n",
    "\n",
    "# Loop through each fold of the dataset\n",
    "for fold_number in range(1, 11):\n",
    "    # Read the X_test and y_test data for the current fold\n",
    "    x_fold = pd.read_parquet(f'{base_path}/{fold_number}/X_test.parquet')\n",
    "    y_fold = pd.read_parquet(f'{base_path}/{fold_number}/y_test.parquet')\n",
    "\n",
    "    # Concatenate the data to the existing test data\n",
    "    concat_fold = pd.concat([x_fold, y_fold], axis=1)\n",
    "    if test is None:\n",
    "        test = concat_fold\n",
    "    else:\n",
    "        test = pd.concat([test, concat_fold])\n",
    "\n",
    "# Set the label for the dataset\n",
    "label = 'count'\n",
    "\n",
    "# Print the information of the training dataset\n",
    "print(train_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this job first, then switch from conda kernel and run the script again excluding this cell\n",
    "\n",
    "# AutoSKLearn Training\n",
    "\n",
    "Auto-sklearn automates the process of model selection and hyperparameter tuning to optimize machine learning pipelines. Save the best pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the autosklearn.regression module\n",
    "import autosklearn.regression\n",
    "\n",
    "# Fit the AutoSklearnRegressor model with the given time budget\n",
    "autosklearn = autosklearn.regression.AutoSklearnRegressor(time_left_for_this_task=time, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the exam dataset\n",
    "autosklearn_pred = autosklearn.predict(exam_X_values)\n",
    "\n",
    "# Save the predictions to a pickle file\n",
    "with open('autosklearn_pred_exam.pkl', 'wb') as f:\n",
    "    pickle.dump(autosklearn_pred, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch kernel to python default before running this\n",
    "### Need to install all required packages and run imports and variables again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install All Required package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn autogluon flaml tpot mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "random_seed = 42\n",
    "time = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAML Training\n",
    "\n",
    "Train a FLAML model which uses efficient hyperparameter optimization to optimize machine learning pipelines. The best pipeline is saved and then used for making predictions. FLAML focuses on pipeline optimization and offers a high degree of automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Instantiate the AutoML object\n",
    "flaml = AutoML()\n",
    "\n",
    "# Define the list of learners to be used\n",
    "learners = ['lgbm', 'rf', 'catboost', 'extra_tree', 'kneighbor']\n",
    "\n",
    "# Fit the AutoML model\n",
    "flaml.fit(\n",
    "    np.array(X_train),  # Training data features\n",
    "    np.array(y_train),  # Training data labels\n",
    "    task=\"regression\",  # Task type is regression\n",
    "    time_budget=time,  # Time budget for the AutoML search\n",
    "    estimator_list=learners,  # List of learners to be used\n",
    "    verbose=1  # Set verbosity level to 1 for progress updates\n",
    ")\n",
    "\n",
    "# Make predictions on the exam dataset\n",
    "flaml_pred = flaml.predict(exam_X_values)\n",
    "\n",
    "# Save the predictions to a pickle file\n",
    "with open(f'flaml_pred_exam.pkl', 'wb') as f:\n",
    "    pickle.dump(flaml_pred, f)\n",
    "#flaml_score = r2_score(test[label], flaml_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon Training\n",
    "\n",
    "Train a model which uses an automated machine learning (AutoML) library Autogluon designed to simplify the process of training and optimizing machine learning models. The predictions are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "# Fit the AutoGluon model with the given time budget\n",
    "gluon = TabularPredictor(label=label, problem_type='regression', eval_metric='r2').fit(train_dataset, time_limit=time, presets='medium_quality', verbosity=1)\n",
    "\n",
    "# Make predictions on the exam dataset\n",
    "gluon_pred = gluon.predict(exam_X_values)\n",
    "\n",
    "# Save the predictions to a pickle file\n",
    "with open('gluon_predictions_EXAM.pkl', 'wb') as f:\n",
    "    pickle.dump(gluon_pred, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT Training\n",
    "Train a TPOT model which uses genetic algorithms to optimize machine learning pipelines. The best pipeline is saved and then used for making predictions. TPOT focuses on pipeline optimization and offers a high degree of automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPOT R2 score: 0.8568653401098989\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "import joblib\n",
    "\n",
    "# Import the necessary libraries\n",
    "\n",
    "# Instantiate the TPOTRegressor\n",
    "tpot = TPOTRegressor(\n",
    "    random_state=random_seed,\n",
    "    n_jobs=-1,              # Utilize all CPU cores\n",
    "    max_time_mins=1,        # Max total time in minutes\n",
    "    max_eval_time_mins=0.2  # Max time per pipeline in minutes\n",
    ")\n",
    "\n",
    "# Fit the TPOTRegressor on the training data\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Save the fitted pipeline\n",
    "joblib.dump(tpot.fitted_pipeline_, \"tpot_pipeline.joblib\")\n",
    "\n",
    "# At prediction time, load the saved pipeline\n",
    "loaded_pipeline = joblib.load(\"tpot_pipeline.joblib\")\n",
    "\n",
    "# Make predictions using the loaded pipeline\n",
    "tpot_predictions = loaded_pipeline.predict(test.drop(columns=[label]))\n",
    "\n",
    "# Calculate the R2 score of TPOT predictions\n",
    "tpot_score = r2_score(test[label], tpot_predictions)\n",
    "\n",
    "# Print the TPOT R2 score\n",
    "print(\"TPOT R2 score:\", tpot_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MLJAR Training\n",
    "Train an MLJAR model using the full dataset. MLJAR performs automated machine learning and provides a model that is evaluated on a test set. It focuses on easy-to-use interfaces and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from supervised import AutoML\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_dataset.drop(columns=[label])\n",
    "y_train = train_dataset[label]\n",
    "\n",
    "# Initialize AutoML for regression\n",
    "mljar_automl_regressor = AutoML(\n",
    "    mode=\"Compete\",\n",
    "    total_time_limit=90,\n",
    "    random_state=random_seed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit AutoML\n",
    "mljar_automl_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "mljar_predict = mljar_automl_regressor.predict(exam_X_values)\n",
    "\n",
    "# Save the model as a pickle file\n",
    "with open('mljar_model_EXAM.pkl', 'wb') as f:\n",
    "    pickle.dump(mljar_automl_regressor, f)\n",
    "\n",
    "# Save the predictions to a pickle file\n",
    "with open('mljar_predictions_EXAM.pkl', 'wb') as f:\n",
    "    pickle.dump(mljar_predict, f)\n",
    "\n",
    "# Print completion messages\n",
    "print(\"Model training completed and model saved as 'mljar_model_EXAM.pkl'.\")\n",
    "print(\"Predictions saved as 'mljar_predictions_EXAM.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.67054082 12.99571632 12.26738305 ... 12.68263049 12.77947423\n",
      " 12.12948934] 2162\n"
     ]
    }
   ],
   "source": [
    "# Load the mljar pred\n",
    "with open('mljar_predictions_EXAM.pkl', 'rb') as f:\n",
    "    mljar_pred = pickle.load(f)\n",
    "\n",
    "#mljar_score = r2_score(test[label][0:len(mljar_pred)], mljar_pred)\n",
    "print(mljar_pred, len(mljar_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980278110859552\n"
     ]
    }
   ],
   "source": [
    "# Load the flaml score\n",
    "with open('flaml_pred_exam.pkl', 'rb') as f:\n",
    "    flaml_pred = pickle.load(f)\n",
    "\n",
    "#flaml_score = r2_score(test[label], flaml_pred)\n",
    "print(flaml_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.77257299 12.98191023 12.3235178  ... 12.63088846 12.71451759\n",
      " 12.15282154] 2162\n"
     ]
    }
   ],
   "source": [
    "# Load the AutoSKLEARN model\n",
    "with open('autosklearn_pred_exam.pkl', 'rb') as f:\n",
    "    autosklearn_pred = pickle.load(f)\n",
    "\n",
    "#autosklearn_score = r2_score(test[label], autosklearn_pred)\n",
    "print(autosklearn_pred, len(autosklearn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gluon predictions\n",
    "with open('gluon_predictions_EXAM.pkl', 'rb') as f:\n",
    "    gluon_pred = pickle.load(f)\n",
    "\n",
    "#gluon_score = r2_score(test[label], gluon_pred)\n",
    "print(gluon_pred, len(gluon_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize a weighted ensemble prediction\n",
    "\n",
    "### TPOT is excluded due to poor performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "\n",
    "# Initial R2 scores\n",
    "r2_flaml = flaml_score.clip(min=0)\n",
    "r2_autosklearn = autosklearn_score.clip(min=0)\n",
    "r2_gluon = eval_gluon['r2'].clip(min=0)\n",
    "r2_mljar = mljar_score.clip(min=0)\n",
    "\n",
    "# Normalize R2 scores to use as initial weights\n",
    "total_r2 = r2_flaml + r2_autosklearn + r2_gluon + r2_mljar\n",
    "initial_weights = [r2_flaml/total_r2, r2_autosklearn/total_r2, r2_gluon/total_r2, r2_mljar/total_r2]\n",
    "\n",
    "def objective(weights):\n",
    "    \"\"\"\n",
    "    Objective function for optimization.\n",
    "    Calculates the ensemble prediction using the given weights and returns the negative R2 score.\n",
    "    \"\"\"\n",
    "    w1, w2, w3, w4 = weights\n",
    "    ensemble_pred = (w1 * flaml_pred + w2 * autosklearn_pred + w3 * gluon_pred +\n",
    "                     w4 * mljar_pred) / (w1 + w2 + w3 + w4)\n",
    "    return -r2_score(test[label], ensemble_pred)\n",
    "\n",
    "# Define the search space\n",
    "space = [Real(max(0, w-0.3), min(1, w+0.3), name=f'w{i+1}') for i, w in enumerate(initial_weights)]\n",
    "# Set up the optimization\n",
    "res = gp_minimize(objective, space, n_calls=50, random_state=random_seed, x0=initial_weights)\n",
    "\n",
    "# Start from the initial weights\n",
    "\n",
    "best_weights = res.x\n",
    "total = sum(best_weights)\n",
    "normalized_weights = [w/total for w in best_weights]\n",
    "w1, w2, w3, w4 = normalized_weights\n",
    "\n",
    "optimal_ensemble = (w1 * flaml_pred + w2 * autosklearn_pred + w3 * gluon_pred +\n",
    "                    w4 * mljar_pred) / (w1 + w2 + w3 + w4)\n",
    "optimal_r2 = r2_score(test[label], optimal_ensemble)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the final ensemble-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of the final exam X dataset\n",
    "\n",
    "# Ensemble to use from testing: AutoSklearn: 0.067, Gluon: 0.584, TPOT: 0.0000, MLJAR: 0.34891\n",
    "\n",
    "final_exam_pred = (0.067 * autosklearn_pred + 0.584 * gluon_pred + 0.349 * mljar_pred)\n",
    "\n",
    "# Convert to numpy array\n",
    "predictions_array = final_exam_pred.to_numpy()\n",
    "\n",
    "np.save('final_test_preds.npy', predictions_array)\n",
    "\n",
    "loaded_preds = np.load('final_test_preds.npy')\n",
    "print(loaded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare it with a longer gluon run (benchmark)\n",
    "\n",
    "gluon_benchmark = TabularPredictor(label=label, problem_type='regression', eval_metric='r2').fit(train_dataset, time_limit=180, presets='medium_quality', verbosity=1)\n",
    "benchmark_pred = gluon_benchmark.predict(test.drop(columns=[label]))\n",
    "eval_benchmark = gluon_benchmark.evaluate(test)\n",
    "\n",
    "print('A 180 sec autogluon run on same dataset.', eval_benchmark['r2'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
